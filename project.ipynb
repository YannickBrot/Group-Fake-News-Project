{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from cleantext import clean\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T12:25:40.355012Z",
     "start_time": "2024-03-07T12:25:39.835694Z"
    }
   },
   "id": "55ec2d58591c1ef",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbe4a582bc21300c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca33a99e3830577"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'\n",
    "\n",
    "news_sample = pd.read_csv(url)\n",
    "unique_words_set_before = set()\n",
    "unique_words_set_after_cleaning = set()\n",
    "unique_words_set_after_tokenization = set()\n",
    "unique_words_set_after_stop_word_removal = set()\n",
    "unique_words_set_after_stemming = set()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for content in news_sample['content']:\n",
    "    content = content.split(' ')\n",
    "    unique_words_set_before.update(content)\n",
    "\n",
    "\n",
    "def clean_with_cleantext(text):\n",
    "    text = clean(text,\n",
    "                 lower=True,\n",
    "                 no_urls=True,\n",
    "                 no_emails=True,\n",
    "                 no_numbers=True,\n",
    "                 no_digits=True,\n",
    "                 replace_with_url=\"\",\n",
    "                 replace_with_email=\"\",\n",
    "                 replace_with_number=\"\",\n",
    "                 replace_with_digit=\"\",\n",
    "                 lang=\"en\")\n",
    "    text = re.sub(r'[-\\/]', ' ', text)\n",
    "    text = re.sub(r'[!\\+\\/@#$%^&?!=\\<\\>_*.,€:;\\[\\]\\{\\}\\'\\\"\\´\\¨\\(\\)\\\\]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "news_sample['content'] = news_sample['content'].apply(clean_with_cleantext)\n",
    "\n",
    "for content in news_sample['content']:\n",
    "    after_clean = content.split(' ')\n",
    "    unique_words_set_after_cleaning.update(after_clean)\n",
    "    content = content.lower()\n",
    "    content = nltk.word_tokenize(content)\n",
    "    unique_words_set_after_tokenization.update(content)\n",
    "    content = [token for token in content if token not in stop_words]\n",
    "    unique_words_set_after_stop_word_removal.update(content)\n",
    "    porter = nltk.PorterStemmer()\n",
    "    content = [porter.stem(token) for token in content]\n",
    "    unique_words_set_after_stemming.update(content)\n",
    "\n",
    "len_before = len(unique_words_set_before)\n",
    "len_after_clean = len(unique_words_set_after_cleaning)\n",
    "len_after_token = len(unique_words_set_after_tokenization)\n",
    "len_after_stopwords = len(unique_words_set_after_stop_word_removal)\n",
    "len_after_stemming = len(unique_words_set_after_stemming)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(['Before', 'After cleaning', 'After tokenization', 'After removing stopwords', 'After stemming'],\n",
    "        [len_before, len_after_clean, len_after_token, len_after_stopwords, len_after_stemming])\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('number of unique words')\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()  # Prevent labels from overlapping\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5ceb505435ef5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "656c44478c8d307a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43d61ed342e9b84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Assuming url is correctly defined and points to a valid CSV file\n",
    "#path = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'  # Placeholder URL\n",
    "path = '995,000_rows.csv'  # Placeholder URL\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Initialize an empty dictionary to hold the frequencies and article counts\n",
    "truth_frequencies = {}\n",
    "article_counts = {}\n",
    "\n",
    "# Compile a regular expression pattern for improved performance\n",
    "pattern = re.compile(r'\\btr(?:ue|uth|uly|uthful|uthfulness)\\b', re.IGNORECASE)\n",
    "\n",
    "print('starting loop')\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the article type\n",
    "    arttype = row['type']\n",
    "\n",
    "    # Ensure the content is a string to avoid errors with missing or NaN values\n",
    "    content = str(row['content'])\n",
    "\n",
    "    # Use re.findall() to find all occurrences of the pattern\n",
    "    matches = pattern.findall(content)\n",
    "\n",
    "    # The number of occurrences is the length of the list returned by findall()\n",
    "    true_count = len(matches)\n",
    "\n",
    "    # If the article type is already in the dictionary, update counts\n",
    "    if arttype in truth_frequencies:\n",
    "        truth_frequencies[arttype] += true_count\n",
    "        article_counts[arttype] += 1\n",
    "    else:\n",
    "        # If the article type is not in the dictionary, initialize it\n",
    "        truth_frequencies[arttype] = true_count\n",
    "        article_counts[arttype] = 1\n",
    "\n",
    "print('finished loop')\n",
    "\n",
    "# Calculate the weighted truth frequencies\n",
    "weighted_truth_frequencies = {arttype: truth_frequencies[arttype] / article_counts[arttype] for arttype in\n",
    "                              truth_frequencies}\n",
    "sorted_truth_frequencies = sorted(weighted_truth_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "types = [str(k) for k, v in sorted_truth_frequencies]\n",
    "frequencies = [v for k, v in sorted_truth_frequencies]\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(types, frequencies)\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()  # Prevent labels from overlapping\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98afe69e8f4fcb7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "281ac867fda96603"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "path = '995,000_rows.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Calculate the length of each content\n",
    "df['content_length'] = df['content'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Group by 'type' and calculate the average content length for each type\n",
    "avg_content_length_by_type = df.groupby('type')['content_length'].mean()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 6))\n",
    "avg_content_length_by_type.plot(kind='bar')\n",
    "plt.title('Average Content Length by Type')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Average Content Length')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "586991fc64176fb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb506a453a190986"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m995,000_rows.csv\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;66;03m# Placeholder path\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Read the CSV file into a DataFrame\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(path)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Initialize dictionaries to hold total word lengths and total word counts for each article type\u001B[39;00m\n\u001B[1;32m      8\u001B[0m total_word_lengths \u001B[38;5;241m=\u001B[39m {}\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#path = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'  # Placeholder URL\n",
    "path = '995,000_rows.csv'  # Placeholder path\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Initialize dictionaries to hold total word lengths and total word counts for each article type\n",
    "total_word_lengths = {}\n",
    "total_word_counts = {}\n",
    "\n",
    "print('starting loop')\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the article type\n",
    "    arttype = row['type']\n",
    "\n",
    "    # Ensure the content is a string to avoid errors with missing or NaN values\n",
    "    content = str(row['content'])\n",
    "\n",
    "    # Split the content into words, filtering out empty strings\n",
    "    words = [word for word in content.split() if word]\n",
    "\n",
    "    # Calculate the total length of all words in the content\n",
    "    total_length = sum(len(word) for word in words)\n",
    "    word_count = len(words)\n",
    "\n",
    "    # Update the total word lengths and counts for the article type\n",
    "    if arttype in total_word_lengths:\n",
    "        total_word_lengths[arttype] += total_length\n",
    "        total_word_counts[arttype] += word_count\n",
    "    else:\n",
    "        total_word_lengths[arttype] = total_length\n",
    "        total_word_counts[arttype] = word_count\n",
    "\n",
    "print('finished loop')\n",
    "\n",
    "# Calculate the average word length for each article type\n",
    "average_word_lengths = {arttype: total_word_lengths[arttype] / total_word_counts[arttype]\n",
    "                        for arttype in total_word_lengths}\n",
    "sorted_average_word_lengths = sorted(average_word_lengths.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "types = [str(k) for k, v in sorted_average_word_lengths]\n",
    "averages = [v for k, v in sorted_average_word_lengths]\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(types, averages)\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()  # Prevent labels from overlapping\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T10:06:17.501888Z",
     "start_time": "2024-03-07T10:06:17.327976Z"
    }
   },
   "id": "586b8a64d9f2a25d",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation 4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f142e2cc33469fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to calculate the LIX number\n",
    "def calculate_lix(text):\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    num_sentences = text.count('.') + text.count('?') + text.count('!')\n",
    "    num_long_words = sum(1 for word in words if len(word) > 6)\n",
    "    if num_sentences == 0:  # Avoid division by zero\n",
    "        return 0\n",
    "    lix = (num_words / num_sentences) + (num_long_words / num_words) * 100\n",
    "    return lix\n",
    "\n",
    "\n",
    "# Load your dataset\n",
    "path = '995,000_rows.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Calculate the LIX number for each content\n",
    "df['lix_number'] = df['content'].apply(lambda x: calculate_lix(str(x)))\n",
    "\n",
    "# Group by 'type' and calculate the average LIX number for each type\n",
    "avg_lix_number_by_type = df.groupby('type')['lix_number'].mean()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 6))\n",
    "avg_lix_number_by_type.plot(kind='bar')\n",
    "plt.title('Average LIX Number by Type')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Average LIX Number')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6577de688eaf9e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6e2495adacf946f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Specify the URL of the CSV file\n",
    "url = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'\n",
    "\n",
    "# Use pandas to read the CSV directly from the URL\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Replace URLs\n",
    "    text = re.sub(r'(https?://)[^, \\n]*', '<URL>', text)\n",
    "    text = re.sub(r'(www.)[^, \\n]*', '<URL>', text)\n",
    "    # Replace emails\n",
    "    text = re.sub(r'\\S+@(\\S+\\.)+\\S+', '<EMAIL>', text)\n",
    "    # Replace dates (simple patterns, for demonstration)\n",
    "    text = re.sub(r'(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec) [0-9]{2,4}', '<DATE>', text)\n",
    "    # Replace number\n",
    "    text = re.sub(r'\\d+(,\\d+)*(\\.\\d+)?', '<NUM>', text)\n",
    "    # Replace various consecutive whitespaces with just one\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
    "    text = re.sub(r'\\t{2,}', '\\t', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_with_cleantext(text):\n",
    "    if type(text) != str:\n",
    "        text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec) [0-9]{2,4}', '<DATE>', text)\n",
    "    return clean(text,\n",
    "                 lower=False,\n",
    "                 no_urls=True,\n",
    "                 no_emails=True,\n",
    "                 no_digits=True,\n",
    "                 no_numbers=True,\n",
    "                 replace_with_url=\"<URL>\",\n",
    "                 replace_with_email=\"<EMAIL>\",\n",
    "                 replace_with_number=\"<NUM>\",\n",
    "                 replace_with_digit=\"<NUM>\",\n",
    "                 normalize_whitespace=True\n",
    "                 )\n",
    "\n",
    "\n",
    "df['content'] = df['content'].apply(clean_with_cleantext)\n",
    "\n",
    "df.to_csv(\"sample_cleaned.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72eed46663939c05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c760dfc95cba8ee3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/08/xjbs0s0j72s556nv4_bvymhh0000gn/T/ipykernel_5114/1377455329.py:4: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(url)\n"
     ]
    }
   ],
   "source": [
    "url = 'cleaned.csv'\n",
    "\n",
    "# Use pandas to read the CSV directly from the URL\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "real_types = ['political', 'fake', 'satire', 'reliable', 'conspiracy', 'unreliable', 'bias', 'rumor', 'clickbait',\n",
    "              'hate', 'junksci']\n",
    "df = df[df['type'].notna() & (df['type'].isin(real_types))]\n",
    "\n",
    "\n",
    "def split_dataset(dataset, labels, test_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # Split into train+val and test\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(dataset, labels, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Split train+val into train and val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=test_size / (1 - test_size), random_state=random_state)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "reliable_types = ['reliable', 'political']\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(df['content'], [int(t in reliable_types) for t in df['type'] ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T12:26:31.643773Z",
     "start_time": "2024-03-07T12:25:43.938870Z"
    }
   },
   "id": "e90a28697978d642",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c291017259b7c431"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-07T13:19:45.815912Z",
     "start_time": "2024-03-07T12:31:16.309888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:otc7xz6a) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.036 MB uploaded\\r'), FloatProgress(value=0.02620436206758808, max=1.…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1669d83ef7b499c910db63c46013ecb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">celestial-music-8</strong> at: <a href='https://wandb.ai/team-alpha-chads/datascience_project/runs/otc7xz6a' target=\"_blank\">https://wandb.ai/team-alpha-chads/datascience_project/runs/otc7xz6a</a><br/> View job at <a href='https://wandb.ai/team-alpha-chads/datascience_project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NjQwMjE3MA==/version_details/v1' target=\"_blank\">https://wandb.ai/team-alpha-chads/datascience_project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NjQwMjE3MA==/version_details/v1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240307_132635-otc7xz6a/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:otc7xz6a). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167559722283234, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a1002b49038463eb39c031bc847c628"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.3"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/maxmeldal/dev/Group-Fake-News-Project/wandb/run-20240307_133116-d7cyulb2</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/team-alpha-chads/datascience_project/runs/d7cyulb2' target=\"_blank\">rare-waterfall-9</a></strong> to <a href='https://wandb.ai/team-alpha-chads/datascience_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/team-alpha-chads/datascience_project' target=\"_blank\">https://wandb.ai/team-alpha-chads/datascience_project</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/team-alpha-chads/datascience_project/runs/d7cyulb2' target=\"_blank\">https://wandb.ai/team-alpha-chads/datascience_project/runs/d7cyulb2</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 63\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;66;03m# Try out scikit-learn integration\u001B[39;00m\n\u001B[1;32m     62\u001B[0m labels \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwords\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 63\u001B[0m wandb\u001B[38;5;241m.\u001B[39msklearn\u001B[38;5;241m.\u001B[39mplot_class_proportions(y_train, y_val, labels)\n\u001B[1;32m     64\u001B[0m wandb\u001B[38;5;241m.\u001B[39msklearn\u001B[38;5;241m.\u001B[39mplot_learning_curve(model, X_train_vectors, y_train)\n\u001B[1;32m     65\u001B[0m wandb\u001B[38;5;241m.\u001B[39msklearn\u001B[38;5;241m.\u001B[39mplot_roc(y_val, y_probas, labels)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sklearn/plot/classifier.py:279\u001B[0m, in \u001B[0;36mclass_proportions\u001B[0;34m(y_train, y_test, labels)\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m not_missing \u001B[38;5;129;01mand\u001B[39;00m correct_types:\n\u001B[1;32m    278\u001B[0m     y_train, y_test \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(y_train), np\u001B[38;5;241m.\u001B[39marray(y_test)\n\u001B[0;32m--> 279\u001B[0m     class_proportions_chart \u001B[38;5;241m=\u001B[39m calculate\u001B[38;5;241m.\u001B[39mclass_proportions(y_train, y_test, labels)\n\u001B[1;32m    281\u001B[0m     wandb\u001B[38;5;241m.\u001B[39mlog({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_proportions\u001B[39m\u001B[38;5;124m\"\u001B[39m: class_proportions_chart})\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sklearn/calculate/class_proportions.py:29\u001B[0m, in \u001B[0;36mclass_proportions\u001B[0;34m(y_train, y_test, labels)\u001B[0m\n\u001B[1;32m     22\u001B[0m class_column, dataset_column, count_column \u001B[38;5;241m=\u001B[39m make_columns(\n\u001B[1;32m     23\u001B[0m     class_ids, counts_train, counts_test\n\u001B[1;32m     24\u001B[0m )\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28misinstance\u001B[39m(class_column[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(class_column[\u001B[38;5;241m0\u001B[39m], np\u001B[38;5;241m.\u001B[39minteger)\n\u001B[1;32m     28\u001B[0m ):\n\u001B[0;32m---> 29\u001B[0m     class_column \u001B[38;5;241m=\u001B[39m get_named_labels(labels, class_column)\n\u001B[1;32m     31\u001B[0m table \u001B[38;5;241m=\u001B[39m make_table(class_column, dataset_column, count_column)\n\u001B[1;32m     32\u001B[0m chart \u001B[38;5;241m=\u001B[39m wandb\u001B[38;5;241m.\u001B[39mvisualize(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwandb/class_proportions/v1\u001B[39m\u001B[38;5;124m\"\u001B[39m, table)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sklearn/calculate/class_proportions.py:68\u001B[0m, in \u001B[0;36mget_named_labels\u001B[0;34m(labels, numeric_labels)\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_named_labels\u001B[39m(labels, numeric_labels):\n\u001B[0;32m---> 68\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray([labels[num_label] \u001B[38;5;28;01mfor\u001B[39;00m num_label \u001B[38;5;129;01min\u001B[39;00m numeric_labels])\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sklearn/calculate/class_proportions.py:68\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_named_labels\u001B[39m(labels, numeric_labels):\n\u001B[0;32m---> 68\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray([labels[num_label] \u001B[38;5;28;01mfor\u001B[39;00m num_label \u001B[38;5;129;01min\u001B[39;00m numeric_labels])\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "from joblib import dump, load\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\"),\n",
    "    \"LinearDiscriminantAnalysis\": LinearDiscriminantAnalysis(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "}\n",
    "\n",
    "run = wandb.init(project=\"datascience_project\", config={\"model\": \"LogisticRegression\"})\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#X = vectorizer.fit_transform(df['content'])\n",
    "#y = [int(t in reliable_types) for t in df['type'] ]\n",
    "X_train = X_train.fillna('')\n",
    "X_test = X_test.fillna('')\n",
    "X_val = X_val.fillna('')\n",
    "\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "X_val_vectors = vectorizer.transform(X_val)\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = models[\"LogisticRegression\"]\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "# Get metrics\n",
    "accuracy = cross_val_score(model, X_train_vectors, y_train, cv=kfold, scoring=\"accuracy\").mean()\n",
    "f1_macro = cross_val_score(model, X_train_vectors, y_train, cv=kfold, scoring=\"f1_macro\").mean()\n",
    "neg_log_loss = cross_val_score(model, X_train_vectors, y_train, cv=kfold, scoring=\"neg_log_loss\").mean()\n",
    "\n",
    "# Log the results\n",
    "wandb.log({\"accuracy\": accuracy, \"f1_macro\": f1_macro, \"neg_log_loss\": neg_log_loss})\n",
    "\"\"\"\n",
    "# Plot the training points\n",
    "fig = sns.scatterplot(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    hue=df.target_names[y],\n",
    "    alpha=1.0,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "wandb.log({\"data_scatter\": wandb.Image(fig)})\n",
    "\"\"\"\n",
    "\n",
    "# Train single model on all data\n",
    "model = models[\"LogisticRegression\"]\n",
    "model.fit(X_train_vectors, y_train)\n",
    "y_pred = model.predict(X_val_vectors)\n",
    "y_probas = model.predict_proba(X_val_vectors)\n",
    "\n",
    "# Try out scikit-learn integration\n",
    "labels = ['words']\n",
    "wandb.sklearn.plot_class_proportions(y_train, y_val, labels)\n",
    "wandb.sklearn.plot_learning_curve(model, X_train_vectors, y_train)\n",
    "wandb.sklearn.plot_roc(y_val, y_probas, labels)\n",
    "wandb.sklearn.plot_precision_recall(y_val, y_probas, labels)\n",
    "wandb.sklearn.plot_confusion_matrix(y_val, y_pred, labels)\n",
    "dump(model, 'logistic_regression_model.joblib')\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['logistic_regression_model.joblib']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T13:24:45.067543Z",
     "start_time": "2024-03-07T13:24:45.030298Z"
    }
   },
   "id": "700c94a25392001b",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ab5d625089266d18"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
