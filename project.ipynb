{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from cleantext import clean\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T10:14:56.929954Z",
     "start_time": "2024-03-07T10:14:55.506678Z"
    }
   },
   "id": "55ec2d58591c1ef",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbe4a582bc21300c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca33a99e3830577"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'\n",
    "\n",
    "news_sample = pd.read_csv(url)\n",
    "unique_words_set_before = set()\n",
    "unique_words_set_after_cleaning = set()\n",
    "unique_words_set_after_tokenization = set()\n",
    "unique_words_set_after_stop_word_removal = set()\n",
    "unique_words_set_after_stemming = set()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for content in news_sample['content']:\n",
    "    content = content.split(' ')\n",
    "    unique_words_set_before.update(content)\n",
    "\n",
    "\n",
    "def clean_with_cleantext(text):\n",
    "    text = clean(text,\n",
    "                 lower=True,\n",
    "                 no_urls=True,\n",
    "                 no_emails=True,\n",
    "                 no_numbers=True,\n",
    "                 no_digits=True,\n",
    "                 replace_with_url=\"\",\n",
    "                 replace_with_email=\"\",\n",
    "                 replace_with_number=\"\",\n",
    "                 replace_with_digit=\"\",\n",
    "                 lang=\"en\")\n",
    "    text = re.sub(r'[-\\/]', ' ', text)\n",
    "    text = re.sub(r'[!\\+\\/@#$%^&?!=\\<\\>_*.,€:;\\[\\]\\{\\}\\'\\\"\\´\\¨\\(\\)\\\\]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "news_sample['content'] = news_sample['content'].apply(clean_with_cleantext)\n",
    "\n",
    "for content in news_sample['content']:\n",
    "    after_clean = content.split(' ')\n",
    "    unique_words_set_after_cleaning.update(after_clean)\n",
    "    content = content.lower()\n",
    "    content = nltk.word_tokenize(content)\n",
    "    unique_words_set_after_tokenization.update(content)\n",
    "    content = [token for token in content if token not in stop_words]\n",
    "    unique_words_set_after_stop_word_removal.update(content)\n",
    "    porter = nltk.PorterStemmer()\n",
    "    content = [porter.stem(token) for token in content]\n",
    "    unique_words_set_after_stemming.update(content)\n",
    "\n",
    "len_before = len(unique_words_set_before)\n",
    "len_after_clean = len(unique_words_set_after_cleaning)\n",
    "len_after_token = len(unique_words_set_after_tokenization)\n",
    "len_after_stopwords = len(unique_words_set_after_stop_word_removal)\n",
    "len_after_stemming = len(unique_words_set_after_stemming)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(['Before', 'After cleaning', 'After tokenization', 'After removing stopwords', 'After stemming'],\n",
    "        [len_before, len_after_clean, len_after_token, len_after_stopwords, len_after_stemming])\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('number of unique words')\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout() # Prevent labels from overlapping\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5ceb505435ef5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "656c44478c8d307a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43d61ed342e9b84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Assuming url is correctly defined and points to a valid CSV file\n",
    "#path = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'  # Placeholder URL\n",
    "path = '995,000_rows.csv'  # Placeholder URL\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Initialize an empty dictionary to hold the frequencies and article counts\n",
    "truth_frequencies = {}\n",
    "article_counts = {}\n",
    "\n",
    "# Compile a regular expression pattern for improved performance\n",
    "pattern = re.compile(r'\\btr(?:ue|uth|uly|uthful|uthfulness)\\b', re.IGNORECASE)\n",
    "\n",
    "print('starting loop')\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the article type\n",
    "    arttype = row['type']\n",
    "\n",
    "    # Ensure the content is a string to avoid errors with missing or NaN values\n",
    "    content = str(row['content'])\n",
    "\n",
    "    # Use re.findall() to find all occurrences of the pattern\n",
    "    matches = pattern.findall(content)\n",
    "\n",
    "    # The number of occurrences is the length of the list returned by findall()\n",
    "    true_count = len(matches)\n",
    "\n",
    "    # If the article type is already in the dictionary, update counts\n",
    "    if arttype in truth_frequencies:\n",
    "        truth_frequencies[arttype] += true_count\n",
    "        article_counts[arttype] += 1\n",
    "    else:\n",
    "        # If the article type is not in the dictionary, initialize it\n",
    "        truth_frequencies[arttype] = true_count\n",
    "        article_counts[arttype] = 1\n",
    "\n",
    "print('finished loop')\n",
    "\n",
    "# Calculate the weighted truth frequencies\n",
    "weighted_truth_frequencies = {arttype: truth_frequencies[arttype] / article_counts[arttype] for arttype in\n",
    "                              truth_frequencies}\n",
    "sorted_truth_frequencies = sorted(weighted_truth_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "types = [str(k) for k, v in sorted_truth_frequencies]\n",
    "frequencies = [v for k, v in sorted_truth_frequencies]\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(types, frequencies)\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()  # Prevent labels from overlapping\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98afe69e8f4fcb7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "281ac867fda96603"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "path = '995,000_rows.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Calculate the length of each content\n",
    "df['content_length'] = df['content'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Group by 'type' and calculate the average content length for each type\n",
    "avg_content_length_by_type = df.groupby('type')['content_length'].mean()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 6))\n",
    "avg_content_length_by_type.plot(kind='bar')\n",
    "plt.title('Average Content Length by Type')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Average Content Length')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "586991fc64176fb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb506a453a190986"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m995,000_rows.csv\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;66;03m# Placeholder path\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Read the CSV file into a DataFrame\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(path)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Initialize dictionaries to hold total word lengths and total word counts for each article type\u001B[39;00m\n\u001B[1;32m      8\u001B[0m total_word_lengths \u001B[38;5;241m=\u001B[39m {}\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#path = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'  # Placeholder URL\n",
    "path = '995,000_rows.csv'  # Placeholder path\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Initialize dictionaries to hold total word lengths and total word counts for each article type\n",
    "total_word_lengths = {}\n",
    "total_word_counts = {}\n",
    "\n",
    "print('starting loop')\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the article type\n",
    "    arttype = row['type']\n",
    "\n",
    "    # Ensure the content is a string to avoid errors with missing or NaN values\n",
    "    content = str(row['content'])\n",
    "\n",
    "    # Split the content into words, filtering out empty strings\n",
    "    words = [word for word in content.split() if word]\n",
    "\n",
    "    # Calculate the total length of all words in the content\n",
    "    total_length = sum(len(word) for word in words)\n",
    "    word_count = len(words)\n",
    "\n",
    "    # Update the total word lengths and counts for the article type\n",
    "    if arttype in total_word_lengths:\n",
    "        total_word_lengths[arttype] += total_length\n",
    "        total_word_counts[arttype] += word_count\n",
    "    else:\n",
    "        total_word_lengths[arttype] = total_length\n",
    "        total_word_counts[arttype] = word_count\n",
    "\n",
    "print('finished loop')\n",
    "\n",
    "# Calculate the average word length for each article type\n",
    "average_word_lengths = {arttype: total_word_lengths[arttype] / total_word_counts[arttype]\n",
    "                        for arttype in total_word_lengths}\n",
    "sorted_average_word_lengths = sorted(average_word_lengths.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "types = [str(k) for k, v in sorted_average_word_lengths]\n",
    "averages = [v for k, v in sorted_average_word_lengths]\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(types, averages)\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()  # Prevent labels from overlapping\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T10:06:17.501888Z",
     "start_time": "2024-03-07T10:06:17.327976Z"
    }
   },
   "id": "586b8a64d9f2a25d",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation 4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f142e2cc33469fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to calculate the LIX number\n",
    "def calculate_lix(text):\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    num_sentences = text.count('.') + text.count('?') + text.count('!')\n",
    "    num_long_words = sum(1 for word in words if len(word) > 6)\n",
    "    if num_sentences == 0:  # Avoid division by zero\n",
    "        return 0\n",
    "    lix = (num_words / num_sentences) + (num_long_words / num_words) * 100\n",
    "    return lix\n",
    "\n",
    "# Load your dataset\n",
    "path = '995,000_rows.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Calculate the LIX number for each content\n",
    "df['lix_number'] = df['content'].apply(lambda x: calculate_lix(str(x)))\n",
    "\n",
    "# Group by 'type' and calculate the average LIX number for each type\n",
    "avg_lix_number_by_type = df.groupby('type')['lix_number'].mean()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 6))\n",
    "avg_lix_number_by_type.plot(kind='bar')\n",
    "plt.title('Average LIX Number by Type')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Average LIX Number')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6577de688eaf9e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6e2495adacf946f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Specify the URL of the CSV file\n",
    "url = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'\n",
    "\n",
    "# Use pandas to read the CSV directly from the URL\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Replace URLs\n",
    "    text = re.sub(r'(https?://)[^, \\n]*', '<URL>', text)\n",
    "    text = re.sub(r'(www.)[^, \\n]*', '<URL>', text)\n",
    "    # Replace emails\n",
    "    text = re.sub(r'\\S+@(\\S+\\.)+\\S+', '<EMAIL>', text)\n",
    "    # Replace dates (simple patterns, for demonstration)\n",
    "    text = re.sub(r'(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec) [0-9]{2,4}', '<DATE>', text)\n",
    "    # Replace number\n",
    "    text = re.sub(r'\\d+(,\\d+)*(\\.\\d+)?', '<NUM>', text)\n",
    "    # Replace various consecutive whitespaces with just one\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
    "    text = re.sub(r'\\t{2,}', '\\t', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_with_cleantext(text):\n",
    "    if type(text) != str:\n",
    "        text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec) [0-9]{2,4}', '<DATE>', text)\n",
    "    return clean(text,\n",
    "                 lower=False,\n",
    "                 no_urls=True,\n",
    "                 no_emails=True,\n",
    "                 no_digits=True,\n",
    "                 no_numbers=True,\n",
    "                 replace_with_url=\"<URL>\",\n",
    "                 replace_with_email=\"<EMAIL>\",\n",
    "                 replace_with_number=\"<NUM>\",\n",
    "                 replace_with_digit=\"<NUM>\",\n",
    "                 normalize_whitespace=True\n",
    "                 )\n",
    "\n",
    "\n",
    "df['content'] = df['content'].apply(clean_with_cleantext)\n",
    "\n",
    "df.to_csv(\"sample_cleaned.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72eed46663939c05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c760dfc95cba8ee3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/08/xjbs0s0j72s556nv4_bvymhh0000gn/T/ipykernel_99853/3758288941.py:4: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(url)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995000\n",
      "903679\n"
     ]
    }
   ],
   "source": [
    "url = 'cleaned.csv'\n",
    "\n",
    "# Use pandas to read the CSV directly from the URL\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "real_types = ['political', 'fake', 'satire', 'reliable', 'conspiracy', 'unreliable', 'bias', 'rumor', 'clickbait', 'hate', 'junksci']\n",
    "df = df[df['type'].notna() & (df['type'].isin(real_types))]\n",
    "\n",
    "\n",
    "def split_dataset(dataset, test_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # Split into train+val and test\n",
    "    train_val, test = train_test_split(dataset, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Split train+val into train and val\n",
    "    train, val = train_test_split(train_val, test_size=test_size/(1-test_size), random_state=random_state)\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "train, val, test = split_dataset(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T10:35:57.675969Z",
     "start_time": "2024-03-07T10:35:14.545880Z"
    }
   },
   "id": "e90a28697978d642",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c291017259b7c431"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-07T10:32:34.717501Z",
     "start_time": "2024-03-07T10:32:33.145890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "722943 90368 90368\n",
      "330566\n",
      "392377\n"
     ]
    }
   ],
   "source": [
    "reliable_types = ['reliable', 'political']\n",
    "train_reliable = train[train['type'].isin(reliable_types)]\n",
    "train_unreliable = train[~train['type'].isin(reliable_types)]\n",
    "\n",
    "val_reliable = val[val['type'].isin(reliable_types)]\n",
    "val_unreliable = val[~val['type'].isin(reliable_types)]\n",
    "\n",
    "test_reliable = test[test['type'].isin(reliable_types)]\n",
    "test_unreliable = test[~test['type'].isin(reliable_types)]\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2df9bf56307ab575"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
